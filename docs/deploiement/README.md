# DÃ©ploiement

Le projet Albert est composÃ© de plusieurs services Ã  dÃ©ployer :
- pyalbert
- models
- api

Pour cela vous devez d'abord disposez d'un environment rÃ©pondants aux exigences requises ([Requirements](#requirements)). Puis vous disposez de maniÃ¨re de dÃ©ployer le projet Albert, sans Docker ([DÃ©ploiement sans Docker](#dÃ©ploiement-sans-docker)) ou avec ([DÃ©ploiement avec Docker](#dÃ©ploiement-avec-docker)). **Le projet est conÃ§u pour Ãªtre dÃ©ployer dans un pipeline de CI/CD Gitlab avec Docker.**

**Tables des matiÃ¨res**

[[_TOC_]]

## Requirements

Le projet est concÃ§u pour fonctionner sur l'environnement Linux Ubuntu 22.04 LTS. De plus, les packages sont nÃ©cessaires :

* jq
* python3.10
* python3.10-venv
* nvidia-driver-535
* nvidia-cuda-toolkit
* nvidia-cuda-toolkit-gcc

*Pour docker :*
* nvidia-container-toolkit
* docker-ce
* docker-ce-cli
* containerd.io
* docker-buildx-plugin
* docker-compose-plugin

Pour un dÃ©ploiement en production vous pouvez utiliser le script [init_vm.sh](../../utils/init_vm.sh) pour configurer l'environnement nÃ©cessaire au projet Albert. Copiez le script sur le serveur et exÃ©cutez la commande suivante :

```bash
bash ./init_vm.sh
```

Ce script permet d'installer les packages nÃ©cessaires ainsi que de crÃ©er un utilisation *gitlab* qui sera nÃ©cessaires pour le dÃ©ploiement de la pipeline de CI/CD. Pour exÃ©cuter le script il est nÃ©cessaire d'exporter prÃ©alablement les variables suivantes :
* `GITLAB_PASSWORD` (mot de passe de l'utilisateur *gitlab*)
* `GITLAB_SSH_PUBLIC_KEY` (clef public qui sera ajoutÃ© Ã  l'utilisateur *gitlab*)

## DÃ©ploiement sans Docker

* Clonez le repository

	```bash
	git clone git@gitlab.com:etalab-datalab/llm/albert-backend.git albert-backend
	```

* CrÃ©ez un environnement virtuel python et l'activer

	```bash
	mkdir albert && python3 -m venv albert && source albert/bin/activate
	```

	> âš ï¸ Vous devez crÃ©er cet environment avec Python 3.10.

### Pyalbert 

* Installez les packages nÃ©cessaires

	```bash
	pip install -r albert-backend/pyalbert/requirements.txt
	```

* Ajoutez pyalbert aux librairies de votre environment virtuel

	```bash
	ln -s albert-backend/pyalbert albert/lib/python3.10/site-packages
	```

	> âš ï¸ Remplacez la version de Python par celle correspondante Ã  votre environment si celle-ci n'est pas 3.10.

### LLM

* Installez les packages nÃ©cessaires

	```bash
	pip install -r albert-backend/api_vllm/requirements.txt
	```

* Configurez les modÃ¨les Ã  dÃ©ployer dans le fichier [llm_routing_table.json](../../pyalbert/config/llm_routing_table.json)

	Pour plus d'information sur comment configurer ce fichier rendez vous sur la documenntation [models.md](../models.md)

* TÃ©lÃ©chargez les modÃ¨les spÃ©cifiez dans le fichier de configuration

	```bash
	python albert-backend/pyalbert/albert.py download_model --storage-dir STORAGE_PATH --env ENV
	```

	> ðŸ’¡ Remplacez STORAGE_PATH par l'emplacement oÃ¹ vous souhaitez stocker les modÃ¨les et ENV par la valeur que vous avez mentionnÃ©e dans le fichier de configuration.

#### GPT4All

TO DO


#### VLLM

 * Lancer l'API du modÃ¨le

	Pour chaque modÃ¨le vous pouvez dÃ©ployer une API pour intÃ©ragir. Commencez par dÃ©finir l'emplacement des modÃ¨les dans une variable *storage_path*.
	
	Puis sÃ©lectionner un modÃ¨le parmi ceux dÃ©finit le fichier de configuration :

	```bash
	routing_table=albert-backend/pyalbert/config/llm_routing_table.json
	models=$(jq -r 'keys[]' $routing_table)

	id=$(echo "$models" | sed -n '1p')
	```

	> âš ï¸ *1* correspond au l'index du modÃ¨le dans le fichier de configuration (ici c'est le premier modÃ¨le qui est sÃ©lectionnÃ©). Remplacez ce chiffre pour sÃ©lectionner un autre model

	```bash
    model=$(jq -r '.["'$id'"] | .model' $routing_table)
    port=$(jq -r '.["'$id'"] | .port' $routing_table)
    gpu_mem_use=$(jq -r '.["'$id'"] | .gpu_mem_use' $routing_table)
    tensor_parralel_size=$(jq -r '.["'$id'"] | .tensor_parralel_size' $routing_table)
	model=${storage_path}/${id}

	python albert-backend/api_vllm/app.py --host=0.0.0.0 --port=$port --model=$model --tensor-parallel-size $tensor_parralel_size --gpu-memory-utilization $gpu_mem_use
	```

### API
	
## Installation avec Docker

L'installation avec Docker se fait dans le cadre d'un pipeline de CI/CD Gitlab. RefÃ©rez-vous au fichier [.gitlab-ci.yml](../../.gitlab-ci.yml) pour plus d'information sur les Ã©tapes de dÃ©ploiement rÃ©alisÃ©e. Afin d'exÃ©cuter cette pipeline il est nÃ©cessaire de configurer au prÃ©alable certaines variables d'environnement dans Gitlab. Pour cela rendez vous sur la documentation [environments.md](environments.md).

Les Ã©tapes de CI/CD sont dÃ©crites schÃ©matiquement ici :

```mermaid
---
title: "Albert deployment flow"
---
graph TD

subgraph VLLM["VLLM"]
    job_vllm_build["build"]
    -.-> job_vllm_setup["setup\n[pyalbert/albert.py]\ndownload_models"]
    -.-> job_vllm_deploy["deploy\n(manual)"]
    -.-> job_vllm_test["test"]
end

subgraph API["API"]
    job_api_build["build"]
    -.-> job_api_setup["setup\n[pyalbert/albert.py]\ncreate_whitelist"]
    -.-> job_api_deploy["deploy\n(manual)"]
    -.-> job_api_test["test"]

end

job_pre["link gpu"]
job_post["unlink gpu"]

job_pre -.-> |"only staging"| VLLM
job_pre -.-> |"only staging"| API
VLLM -.-> |"only staging"| job_post
API -.-> |"only staging"| job_post
```